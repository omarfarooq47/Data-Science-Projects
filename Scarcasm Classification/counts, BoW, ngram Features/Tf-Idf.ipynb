{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  sarcastic  \\\n",
       "0           0  The only thing I got from college is a caffein...          1   \n",
       "1           1  I love it when professors draw a big question ...          1   \n",
       "2           2  Remember the hundred emails from companies whe...          1   \n",
       "3           3  Today my pop-pop told me I was not “forced” to...          1   \n",
       "4           4  @VolphanCarol @littlewhitty @mysticalmanatee I...          1   \n",
       "\n",
       "   sarcasm  irony  satire  understatement  overstatement  rhetorical_question  \n",
       "0      0.0    1.0     0.0             0.0            0.0                  0.0  \n",
       "1      1.0    0.0     0.0             0.0            0.0                  0.0  \n",
       "2      0.0    1.0     0.0             0.0            0.0                  0.0  \n",
       "3      1.0    0.0     0.0             0.0            0.0                  0.0  \n",
       "4      1.0    0.0     0.0             0.0            0.0                  0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Sarcasm Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3468, 9)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3467, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['tweet'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df['tweet'])\n",
    "y = df[\"sarcastic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive-Bayes Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.713256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.713256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.832632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Naive-Bayes Eval. Metric     Score\n",
       "0         accuracy_score:   0.713256\n",
       "1        precision_score:   0.713256\n",
       "2           recall_score:   1.000000\n",
       "3               f1_score:   0.832632"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "dic = {\n",
    "    \"Naive-Bayes Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.716138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.796949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.716138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.600587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Logistic Regression Eval. Metric     Score\n",
       "0                 accuracy_score:   0.716138\n",
       "1                precision_score:   0.796949\n",
       "2                   recall_score:   0.716138\n",
       "3                       f1_score:   0.600587"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "dic = {\n",
    "    \"Logistic Regression Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.716138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.725708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.716138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.603157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Random Forest Eval. Metric     Score\n",
       "0           accuracy_score:   0.716138\n",
       "1          precision_score:   0.725708\n",
       "2             recall_score:   0.716138\n",
       "3                 f1_score:   0.603157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rF = RandomForestClassifier()\n",
    "rF.fit(X_train, y_train)\n",
    "y_pred = rF.predict(X_test)\n",
    "dic = {\n",
    "    \"Random Forest Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.713256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.713256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.832632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SVM Eval. Metric     Score\n",
       "0   accuracy_score:   0.713256\n",
       "1  precision_score:   0.713256\n",
       "2     recall_score:   1.000000\n",
       "3         f1_score:   0.832632"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "sv = SVC()\n",
    "sv.fit(X_train, y_train)\n",
    "y_pred = sv.predict(X_test)\n",
    "dic = {\n",
    "    \"SVM Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perceptron Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.675793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.634456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.675793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.645619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Perceptron Eval. Metric     Score\n",
       "0        accuracy_score:   0.675793\n",
       "1       precision_score:   0.634456\n",
       "2          recall_score:   0.675793\n",
       "3              f1_score:   0.645619"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "tron = Perceptron()\n",
    "tron.fit(X_train, y_train)\n",
    "y_pred = tron.predict(X_test)\n",
    "dic = {\n",
    "    \"Perceptron Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df[df.sarcastic == 1]['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sarcasm  irony  satire  understatement  overstatement  rhetorical_question\n",
       "0      0.0    1.0     0.0             0.0            0.0                  0.0\n",
       "1      1.0    0.0     0.0             0.0            0.0                  0.0\n",
       "2      0.0    1.0     0.0             0.0            0.0                  0.0\n",
       "3      1.0    0.0     0.0             0.0            0.0                  0.0\n",
       "4      1.0    0.0     0.0             0.0            0.0                  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[df.sarcastic == 1].copy().drop(columns = ['Unnamed: 0', 'tweet', 'sarcastic'])\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(867, 4080) (867, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.701149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.723477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.850575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.781895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Random Forest Eval. Metric     Score\n",
       "0           accuracy_score:   0.701149\n",
       "1          precision_score:   0.723477\n",
       "2             recall_score:   0.850575\n",
       "3                 f1_score:   0.781895"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rF = RandomForestClassifier()\n",
    "rF.fit(X_train, y_train)\n",
    "y_pred = rF.predict(X_test)\n",
    "dic = {\n",
    "    \"Random Forest Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DecisionTree Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.540230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.752637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.764368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.758337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DecisionTree Eval. Metric     Score\n",
       "0          accuracy_score:   0.540230\n",
       "1         precision_score:   0.752637\n",
       "2            recall_score:   0.764368\n",
       "3                f1_score:   0.758337"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtC = DecisionTreeClassifier()\n",
    "dtC.fit(X_train, y_train)\n",
    "y_pred = dtC.predict(X_test)\n",
    "dic = {\n",
    "    \"DecisionTree Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtraTreeClassifier Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.614943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.776314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.816092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.792088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ExtraTreeClassifier Eval. Metric     Score\n",
       "0                 accuracy_score:   0.614943\n",
       "1                precision_score:   0.776314\n",
       "2                   recall_score:   0.816092\n",
       "3                       f1_score:   0.792088"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "etC = ExtraTreeClassifier()\n",
    "etC.fit(X_train, y_train)\n",
    "y_pred = etC.predict(X_test)\n",
    "dic = {\n",
    "    \"ExtraTreeClassifier Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtraTreesClassifier Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.695402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.722743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.844828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.779031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ExtraTreesClassifier Eval. Metric     Score\n",
       "0                  accuracy_score:   0.695402\n",
       "1                 precision_score:   0.722743\n",
       "2                    recall_score:   0.844828\n",
       "3                        f1_score:   0.779031"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etsC = ExtraTreesClassifier()\n",
    "etsC.fit(X_train, y_train)\n",
    "y_pred = etsC.predict(X_test)\n",
    "dic = {\n",
    "    \"ExtraTreesClassifier Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNeighbors Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.683908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.754635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.782402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  KNeighbors Eval. Metric     Score\n",
       "0        accuracy_score:   0.683908\n",
       "1       precision_score:   0.754635\n",
       "2          recall_score:   0.833333\n",
       "3              f1_score:   0.782402"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "dic = {\n",
    "    \"KNeighbors Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLPClassifier Eval. Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_score:</td>\n",
       "      <td>0.683908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score:</td>\n",
       "      <td>0.721248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score:</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score:</td>\n",
       "      <td>0.773250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MLPClassifier Eval. Metric     Score\n",
       "0           accuracy_score:   0.683908\n",
       "1          precision_score:   0.721248\n",
       "2             recall_score:   0.833333\n",
       "3                 f1_score:   0.773250"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "dic = {\n",
    "    \"MLPClassifier Eval. Metric\": [\"accuracy_score: \", \n",
    "               \"precision_score: \", \n",
    "               \"recall_score: \", \n",
    "               \"f1_score: \"],\n",
    "    \"Score\": [accuracy_score(y_test, y_pred), \n",
    "              precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)), \n",
    "              f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))]\n",
    "}\n",
    "pd.DataFrame(dic).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
